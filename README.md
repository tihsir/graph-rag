# Graph-RAG Evaluation Framework

Long-form QA evaluation comparing three RAG approaches using LLM-as-a-Judge.

## ğŸ¯ Overview

This framework evaluates and compares:
1. **Vanilla RAG** - Simple semantic retrieval (FAISS)
2. **KG-RAG** - Knowledge Graph retrieval (ChromaDB + SPOKE)
3. **SARG+ORAG** - Hybrid system with triple extraction and self-verification

## ğŸ“Š Results Summary

| System | Score | Rank |
|--------|-------|------|
| **SARG+ORAG** | **8.88/10** | ğŸ¥‡ |
| **KG-RAG** | **8.67/10** | ğŸ¥ˆ |
| **Vanilla RAG** | **8.00/10** | ğŸ¥‰ |

## ğŸ”§ Features

### (a) Chain of Reasoning
Shows all triples extracted and their usage order in the final answer.

### (b) Triple Deduplication  
Merges KG triples (Group A) + Semantic triples (Group B), removes duplicates.

### (c) Debug Evidence Display
Optionally displays raw KG and semantic context retrieved.

### (d) Enhanced Evaluation Rubric
8 metrics with rankings:
- Accuracy, Completeness, Relevance, Coherence
- Conciseness, Evidence Usage, Reasoning Depth, Factual Grounding

## ğŸ“ Project Structure

```
graph-rag-eval/
â”œâ”€â”€ kg_rag/
â”‚   â”œâ”€â”€ utility.py                 # Core retrieval functions
â”‚   â”œâ”€â”€ config_loader.py           # Configuration loading
â”‚   â””â”€â”€ rag_based_generation/
â”‚       â””â”€â”€ GPT/
â”‚           â”œâ”€â”€ run_longform_eval.py    # Basic 3-system comparison
â”‚           â””â”€â”€ run_longform_eval_v2.py # Enhanced with all features
â”œâ”€â”€ data/
â”‚   â””â”€â”€ my_results/
â”‚       â”œâ”€â”€ longform_eval_summary.md
â”‚       â””â”€â”€ enhanced_longform_eval_v2.csv
â”œâ”€â”€ config.yaml                    # System configuration
â”œâ”€â”€ system_prompts.yaml            # LLM prompts
â””â”€â”€ README.md
```

## ğŸš€ Usage

### Basic Evaluation (3 systems)
```bash
export GOOGLE_API_KEY="your-api-key"
python -m kg_rag.rag_based_generation.GPT.run_longform_eval
```

### Enhanced Evaluation (with all features)
```bash
python -m kg_rag.rag_based_generation.GPT.run_longform_eval_v2
```

### Configuration
Edit `run_longform_eval_v2.py`:
```python
DEBUG_MODE = True          # Show retrieved evidence (c)
SHOW_REASONING_CHAIN = True # Show triple chain (a)
```

## ğŸ“‹ Triple Extraction Pipeline

```
Question
    â”‚
    â”œâ”€â”€â–º KG Retrieval â”€â”€â–º KG Triples (Group A)
    â”‚                            â”‚
    â””â”€â”€â–º Semantic Retrieval â”€â”€â–º Semantic Triples (Group B)
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Merge & Deduplicate â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Final Triples     â”‚
                    â”‚   + Raw Context     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  LLM Answer Gen     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Self-Verification  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          Final Answer
```

## ğŸ“ˆ Evaluation Metrics

| Metric | Description | Weight |
|--------|-------------|--------|
| Accuracy | Medical facts correct? | 20% |
| Completeness | Covers all key points? | 20% |
| Relevance | Addresses the question? | 15% |
| Coherence | Well-organized? | 10% |
| Conciseness | Appropriately detailed? | 10% |
| Evidence Usage | Uses retrieved evidence? | 10% |
| Reasoning Depth | Shows clear reasoning? | 10% |
| Factual Grounding | Based on evidence? | 5% |

## ğŸ”— Dependencies

- `google-generativeai` - Gemini API
- `langchain` - LLM framework
- `chromadb` - Vector store for KG
- `faiss` - Semantic search
- `sentence-transformers` - Embeddings
- `pandas` - Data handling

## ğŸ“ License

MIT

